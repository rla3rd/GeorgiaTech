\documentclass[12pt]{article}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath, amsthm}
\RequirePackage[colorlinks]{hyperref}
\usepackage[lined,boxed,linesnumbered,commentsnumbered]{algorithm2e}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

% Commands
\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}

\newcommand{\snehaedit}[1]{\textcolor{green}{\emph{[Sneha: #1]}}}
\newcommand{\hsedit}[1]{\textcolor{magenta}{\emph{[Hang: #1]}}}
\newcommand{\meeraedit}[1]{\textcolor{red}{\emph{[Meera: #1]}}}
\newcommand{\prpedit}[1]{\textcolor{blue}{\emph{[Prp: #1]}}}


\begin{document}

\title{Richard Albright\\CSE 6250\\
Spring 2021 \\
Homework 2 \\ 
\small}
\date{}
\maketitle


{1.1 Batch Gradient Descent}


The negative log-likelihood can be calculated according to
\begin{center}
$NLL\left (D, \mathbf{w} \right ) = -\sum_{i=1}^{N} \left [ \left ( 1 - y_i \right ) \log(1-\sigma(\mathbf{w}^T\mathbf{x}_i)) + y_i\log \sigma(\mathbf{w}^T\mathbf{x}_i)  \right ]$ \end{center}
The maximum likelihood estimator $\mathbf{w}_{\text{MLE}}$ can be found by solving for $\underset{\mathbf{w}}{\operatorname{arg\min}}$ $NLL$ through an iterative gradient descent procedure.\\

\textbf{a.} Derive the gradient of the negative log-likelihood in terms of $\mathbf{w}$ for this setting. [5 points] 

\subsection{Stochastic Gradient Descent}
If $N$ and $d$ are very large, it may be prohibitively expensive to consider every patient in $D$ before applying an update to $\mathbf{w}$. One alternative is to consider stochastic gradient descent, in which an update is applied after only considering a single patient. \\

\textbf{a.} Show the log likelihood, $l$, of a single $(\mathbf{x}_t, y_t)$ pair. [5 points]
\vspace{5mm}

$NLL\left (D, \mathbf{w} \right ) = -\sum_{i=1}^{N} \left [ \left ( 1 - y_i \right ) \log(1-\sigma(\mathbf{w}^T\mathbf{x}_i)) + y_i\log \sigma(\mathbf{w}^T\mathbf{x}_i)  \right ]$

$NLL\left (D, \mathbf{w} \right ) = -\sum_{i=1}^{N} \left [ \left ( 1 - y_i \right ) (1-\sigma(\mathbf{w}^T\mathbf{x}_i)) + y_i \sigma(\mathbf{w}^T\mathbf{x}_i)  \right ]$
The maximum likelihood estimator $\mathbf{w}_{\text{MLE}}$ 

\textbf{b.} Show how to update the coefficient vector $\mathbf{w}_t$ when you get a patient feature vector $\mathbf{x}_t$ and physician feedback label $y_t$ at time $t$ using $\mathbf{w}_{t-1}$ (assume learning rate $\mathbf{\eta}$ is given). [5 points]

\textbf{c.} What is the time complexity of the update rule from $\mathbf{b}$ if $\mathbf{x}_t$ is very sparse? [2 points]

\textbf{d.} Briefly explain the consequence of using a very large $\mathbf{\eta}$ and very small $\mathbf{\eta}$. [3 points]

\textbf{e.} Show how to update $\mathbf{w}_t$ under the penalty of L2 norm regularization. In other words, update $\mathbf{w}_t$ according to $l - \mu \|\mathbf{w}\|_2^2 $, where $\mathbf{\mu}$ is a constant. What's the time complexity? [5 points]

\section{Programming [70 points]}
First, follow the \href{http://www.sunlab.org/teaching/cse6250/fall2018/env/}{instructions} to install the environment if you haven't done that yet. You will also need the $hw2/data/$ from Canvas. Remember to send relevant data to HDFS in this homework when needed following each step instructions.

\textbf{You will need to use Python 3.6.5 for this homework} which is also the default python version we provide and set the consistent dependencies in 'environment.yml' like what you did in HW1. 


\subsection{Descriptive Statistics [15 points]}
Computing descriptive statistics on the data helps in developing predictive models. In this section, you need to write HIVE code that computes various metrics on the data. A skeleton code is provided as a starting point. \\

The definition of terms used in the result table are described below:
\begin{itemize}
\item \textbf{Event Count [3 points]}: Number of events recorded for a given patient. Note that every line in the input file is an event. 
\item \textbf{Encounter Count [3 points]}: Count of unique dates on which a given patient visited the ICU.
\item \textbf{Record Length [3 points]}: Duration (in number of days) between first event and last event for a given patient.
\item \textbf{Common Diagnosis [2 points]}: 5 most frequently occurring disease.
\item \textbf{Common Laboratory Test [2 points]}: 5 most frequently conducted test.
\item \textbf{Common Medication [2 points]}: 5 most frequently prescribed medications.
\end{itemize}
While counting common diagnoses, lab tests and medications, count all the occurrences of the codes. e.g. if one patient has the same code 3 times, the total count on that code should include all 3. Furthermore, the count is not per patient but per code.

\textbf{a.} Complete \textit{hive/event\_statistics.hql} for computing statistics required in the question. Please be aware that \textbf{\color{red} you are not allowed to change the filename.} 

\textbf{b.} Use \textit{events.csv} and \textit{mortality.csv} provided in \textbf{data} as input and fill Table~\ref{tbl:stat} with actual values [5 points]. We only need the top 5 codes for common diagnoses, labs and medications. Their respective counts are not required. \\

\begin{table}[th]
\centering
\begin{tabular}{@{}l|l|l}
\toprule
Metric & Deceased patients & Alive patients  \\ \hline
Event Count & &  \\ 
1. Average Event Count && \\
2. Max Event Count  &&\\
3. Min Event Count  &&\\ \hline

Encounter Count & &  \\ 
1. Average Encounter Count  &&\\
2. Median Record Count &&\\
3. Max Encounter Count  &&\\
4. Min Encounter Count  &&\\ \hline

Record Length & &  \\ 
1. Average Record Length &&\\
2. Median Record Length &&\\
3. Max Record Length&& \\
4. Min Record Length&& \\ \hline

Common Diagnosis & &  \\ \hline

Common Laboratory Test & &  \\ \hline

Common Medication & &  \\ 
\bottomrule
\end{tabular}
\caption{Descriptive statistics for alive and dead patients\label{tbl:stat}}
\end{table} 

\textbf{Deliverable: code/hive/event\_statistics.hql [10 points]}

\subsection{Transform data [20 points]}
In this problem, we will convert the raw data to standardized format using Pig. Diagnostic, medication and laboratory codes for each patient should be used to construct the feature vector and the feature vector should be represented in \href{http://svmlight.joachims.org/}{SVMLight} format. You will work with \textit{events.csv} and \textit{mortality.csv} files provided in \textbf{data} folder. 

Listed below are a few concepts you need to know before beginning feature construction (for details please refer to lectures). 

\begin{itemize}
\item \textbf{Observation Window:} The time interval containing events you will use to construct your feature vectors. Only events in this window should be considered. The observation window ends on the index date (defined below) and starts 2000 days (including 2000) prior to the index date.
\item \textbf{Prediction Window:} A fixed time interval following the index date where we are observing the patient's mortality outcome. This is to simulate predicting some length of time into the future. Events in this interval should not be included while constructing feature vectors. The size of prediction window is 30 days.
\item \textbf{Index date:} The day on which we will predict the patient's probability of dying during the subsequent prediction window. Events occurring on the index date should be considered within the observation window. Index date is determined as follows:
\begin{itemize}
\item For deceased patients: Index date is 30 days prior to the death date (timestamp field) in \textit{mortality.csv}. 
\item For alive patients: Index date is the last event date in \textit{events.csv} for each alive patient. 
\end{itemize}
\end{itemize}

You will work with the following files in \textit{code/pig} folder
\begin{itemize}
\item \textbf{etl.pig}: Complete this script based on provided skeleton. 
\item \textbf{utils.py}: Implement necessary User Defined Functions (UDF) in Python in this file (optional).
\end{itemize}

In order to convert raw data from events to features, you will need a few steps:
\begin{enumerate}
\item \emph{Compute the index date:} [4 points] Use the definition provided above to compute the index date for all patients. 

\item \emph{Filter events:} [4 points] Consider an observation window (2000 days) and prediction window (30 days).
Remove the events that occur outside the observation window. 

\item \emph{Aggregate events:} [4 points] To create features suitable for machine learning, we will need to aggregate the events for each patient as follows:
  \begin{itemize}
  \item \textbf{count:} occurrence for diagnostics, lab and medication events (i.e. event\_id starting with DRUG, LAB and DIAG respectively) to get their counts.  
  \end{itemize}
  
Each event type will become a feature and we will directly use event\_id as feature name. For example, given below raw event sequence for a patient, \\

\begin{lstlisting}[frame=single]
1053,DIAG319049,Acute respiratory failure,2924-10-08,1.0
1053,DIAG197320,Acute renal failure syndrome,2924-10-08,1.0
1053,DRUG19122121,Insulin,2924-10-08,1.0
1053,DRUG19122121,Insulin,2924-10-11,1.0
1053,LAB3026361,Erythrocytes in Blood,2924-10-08,3.000
1053,LAB3026361,Erythrocytes in Blood,2924-10-08,3.690
1053,LAB3026361,Erythrocytes in Blood,2924-10-09,3.240
1053,LAB3026361,Erythrocytes in Blood,2924-10-10,3.470
\end{lstlisting}

We can get feature value pairs($event\_id$, $value$) for this patient with ID \textit{1053} as \\
\begin{lstlisting}[frame=single]
(DIAG319049, 1)
(DIAG197320, 1)
(DRUG19122121, 2)
(LAB3026361, 4)
\end{lstlisting}


\item \emph{Generate feature mapping:} [4 points]
In above result, you see the feature value as well as feature name ($event\_id$ here). Next, you need to assign an unique identifier for each feature. Sort all unique feature names in ascending alphabetical order and assign continuous feature id starting from 0. Thus above result can be mapped to

\begin{lstlisting}[frame=single]
(1, 1)
(0, 1)
(2, 2)
(3, 4)
\end{lstlisting}

\iffalse
\item \emph{Imputation:} [10 points]
Physicians order lab tests for patient on demand, thus we don't have average lab value as feature if patient don't have such test. This is a missing value problem. For a given feature, if its value is not missing from a few patients(more than 10 patients here in this problem), one can use average value of non-missing to replace missing, this is called the \href{http://missingdata.lshtm.ac.uk/index.php?option=com_content&view=article&id=68:simple-mean-imputation&catid=39:simple-ad-hoc-methods-for-coping-with-missing-data&Itemid=96}{Simple Mean Imputation}.
\fi

\item \emph{Normalization:} [4 points] In machine learning algorithms like logistic regression, it is important to normalize different features into the same scale. Implement 
 
\href{http://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range}{min-max normalization} on your results. (Hint: $min(x_i)$ maps to 0 and $max(x_i)$ 1 for feature $x_i$, $min(x_i)$ is zero for \textbf{count} aggregated features).

\item \emph{Save in  SVMLight format:} If the dimensionality of a feature vector is large but the feature vector is sparse (i.e. it has only a few nonzero elements), sparse representation should be employed. In this problem you will use the provided data for each patient to construct a feature vector and represent the feature vector in \href{http://svmlight.joachims.org/}{SVMLight} format shown below: \\

\begin{lstlisting}[frame=single, language=bash]
<line> .=. <target> <feature>:<value> <feature>:<value> 
<target> .=. 1 | 0
<feature> .=. <integer>
<value> .=. <float>
\end{lstlisting}

The target value and each of the feature/value pairs are separated by a space character. Feature/value pairs MUST be ordered by increasing feature number. Features with value zero can be skipped. For example, the feature vector in SVMLight format will look like: \\

\begin{lstlisting}[frame=single, language=bash]
1 2:0.5 3:0.12 10:0.9 2000:0.3
0 4:1.0 78:0.6 1009:0.2
1 33:0.1 34:0.98 1000:0.8 3300:0.2
1 34:0.1 389:0.32 
\end{lstlisting}

where, 1 or 0 will indicate whether the patient is dead or alive( i.e. the target), and it will be followed by a series of feature-value pairs sorted by the feature index (idx) value. 
\newline
\end{enumerate}
To run your pig script in local mode, you will need the command:  
\begin{lstlisting}[frame=single,language=bash]
sudo pig -x local etl.pig
\end{lstlisting}

\textbf{Deliverable: pig/etl.pig and pig/utils.py [20 points]}

\subsection{SGD Logistic Regression [20 points]}
In this question, you are going to implement your own Logistic Regression classifier in Python using the equations you derived in question \textbf{1.2.e}. To help you get started, we have provided a skeleton code. You will find the relevant code files in \textit{lr} folder. You will train and test a classifier by running 
\begin{enumerate}
\item cat path/to/train/data \textbar{} python train.py -f $<$number of features$>$
\item cat path/to/test/data \textbar{} python test.py 
\end{enumerate}
The training and testing data for this problem will be output from previous Pig ETL problem. \\

To better understand the performance of your classifier, you will need to use standard metrics like AUC. Similarly with Homework 1, we provide \textit{code/environment.yml} which contains a list of libraries needed to setup the environment for this homework. You can use it to create a copy of conda `environment' 

(\href{http://conda.pydata.org/docs/using/envs.html#use-environment-from-file}{http://conda.pydata.org/docs/using/envs.html\#use-environment-from-file}). If you already have your own Python development environment (it should be Python 3.6.5), please refer to this file to find necessary libraries. It will help you install necessary modules for drawing an ROC curve. You may need to modify it if you want to install it somewhere else. Remember to restart the terminal after installation.

\textbf{a.} Update the \textbf{lrsgd.py} file. You are allowed to add extra methods, but please make sure the existing method names and parameters remain unchanged. Use \href{https://docs.python.org/3/library/}{standard modules} only, as we will not guarantee the availability of any third party modules while testing your code. [15 points]

\textbf{b.} Show the ROC curve generated by test.py in this writing report for different learning rates $\eta$ and regularization parameters $\mu$ combination and briefly explain the result. [5 points]
\iffalse
\textbf{c.} [Extra 10 points \textbf{(no partial credit!)}] Implement using the result of question \textbf{1.2.f}, and show the speed up. Test efficiency of your approach using larger data set \textit{training.data}, which has 5675 possible distinct features. Save the code in a new file lrsgd\_fast.py. We will test whether your code can finish witin reasonable amount of time and correctness of trained model. The training and testing data set can be downloaded from:

\begin{lstlisting}[frame=single,language=bash]
http://sunlab.org/download/course/hw2/training.data
http://sunlab.org/download/course/hw2/testing.data
\end{lstlisting}
\textbf{Deliverable: lr/lrsgd.py and optional lr/lrsgd\_fast.py [15 points]}
\fi



\textbf{Deliverable: lr/lrsgd.py [20 points]}

\subsection{Hadoop [15 points]}
In this problem, you are going to train multiple logistic regression classifiers using your implementation of previous problem with Hadoop in parallel. The pseudo code of Mapper and Reducer are listed as Algorithm~\ref{algo_map} and Algorithm~\ref{algo_reduce} respectively. We have already written the code for the reducer for you. Find related files in \textit{lr} folder.

\begin{algorithm}
\SetKwFunction{Random}{Random}\SetKwFunction{Emit}{Emit}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{Ratio of sampling $r$, number of models $M$, input pair $(k,v)$}
\Output{key-value pairs}
\BlankLine
\For{$i\leftarrow 1$ \KwTo $M$}{
$m\leftarrow$\Random\;
\If{$m < r$}{
\Emit($i,v$)
}
}
\caption{Map function}\label{algo_map}
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$(k,v)$}
\Output{Trained model}
Fit model on $v$\;
\caption{Reduce function}\label{algo_reduce}
\end{algorithm}
You need to copy \textit{training} (the output of Pig ETL) into HDFS using command line.
\begin{lstlisting}[language=bash,frame=single]
hdfs dfs -mkdir /hw2
hdfs dfs -put pig/training /hw2  # adjust train path as needed
\end{lstlisting}
\textbf{a.} Complete the \textbf{mapper.py} according to pseudo code. [5 points]\\
You could train 5 ensembles by invoking
\begin{lstlisting}[language=bash,frame=single]
hadoop jar \
   /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
  -D mapreduce.job.reduces=5    \
  -files lr \
  -mapper "python lr/mapper.py -n 5 -r 0.4" \
  -reducer "python lr/reducer.py -f <number of features>" \
  -input /hw2/training \
  -output /hw2/models
\end{lstlisting}
Notice that you could apply other parameters to reducer. To test the performance of ensembles, copy the trained models to local via
\begin{lstlisting}[language=bash,frame=single]
hdfs dfs -get /hw2/models
\end{lstlisting}
\textbf{b.} Complete the \textbf{testensemble.py} to generate the ROC curve. [5 points]
\begin{lstlisting}[language=bash,frame=single]
cat pig/testing/* | python lr/testensemble.py -m models
\end{lstlisting}
\textbf{c.} Compare the performance with that of previous problem and briefly analyze why the difference. [5 points]
\iffalse
\subsection{Zeppelin [10 points]}
\href{http://zeppelin.apache.org/}{Apache Zeppelin} is an web based notebook that enables interactive data analytics (like Jupyter).
Because you can execute your code piecewise interactively, you're encouraged to use this at the initial stage your development for fast prototyping and initial data exploration. Check out the course lab \href{http://www.sunlab.org/teaching/cse6250/spring2018/lab/zeppelin-intro/}{pages} for a brief introduction on how to set it up and use it. Please fill in the TODO section of the JSON file, \texttt{zeppelin\textbackslash bdh\_hw2\_zeppelin.json}. Import this notebook file on Zeppelin first.
For easier start, we will read in the dataset we have been using in the lab, events.csv and mortality.csv, first in this part. Read carefully the provided comments in the Notebook and fill-in the TODO section:

\begin{itemize}
\item \textbf{Event count}: Number of events recorded for a given patient. Note that every line in the input file is an event. [3 points]
\item For the average, maximum and minimum event counts, show the breakdown by dead or alive. Produce a chart like below (please note that values and axis labels here are for illustrative purposes only and maybe different with the actual data): [2 points]\\
\includegraphics[width=\textwidth]{Events.png}
\item \textbf{Encounter count}: Count of unique dates on which a given patient visited the ICU. All the events: DIAG, LAB and DRUG - should be considered as ICU visiting events.[3 points]
\item For the average, maximum and minimum encounter counts, show the breakdown by dead or alive. Produce a chart like below (please note that values and axis labels here are for illustrative purposes only and maybe different with the actual data): [2 points]\\
\includegraphics[width=\textwidth]{Encounters.png}
\end{itemize}
Fill in the indicated TODOs in the notebook using ONLY Scala.\\

Please be aware that \textbf{\color{red} you are NOT allowed to change the filename and any existing function declarations.}\\

The submission bdh\_hw2\_zeppelin.json file should have all cells run and charts visible. Please don't clear the cell output before submission.

\textbf{Deliverable: \texttt{zeppelin\textbackslash bdh\_hw2\_zeppelin.json}[10 points]}
\fi

\subsection{Submission [5 points]}
The folder structure of your submission should be as below. You can display fold structure using \textit{tree} command. All other unrelated files will be discarded during testing.
Please make sure your code can compile/run normally, otherwise you will get full penalty without comments. Organize your files same as the below framework, otherwise you will get 0/5 for Submission part.
\begin{lstlisting}[language=bash,frame=single]
<your gtid>-<your gt account>-hw2
|-- code
| |-- hive
| | \-- event_statistics.hql
| |-- lr
| | |-- lrsgd.py
| | |-- mapper.py
| | \-- testensemble.py
| |-- pig
| |  |-- etl.pig
| |  \-- utils.py
\-- homework2_answer.pdf
\end{lstlisting}

Please create this folder manually, put all the files inside the folder, and create a tar archive using the following command and submit the tar file (keep in mind the folder '$<your GTid>$-$<your GT account>$-hw2' is required before creating the tar file) onto Canvas (Hadoop is not well supported on Gradescope now).
\begin{lstlisting}[language=bash,frame=single]
tar -czvf <your GTid>-<your GT account>-hw2.tar.gz \
  <your GTid>-<your GT account>-hw2
\end{lstlisting}
Example submission: 901234567-gburdell3-hw2.tar.gz
\newline

\noindent Common Errors which are not accepted:
\begin{itemize}
\item Underscore: 901234567\_gburdell3\_hw2.tar.gz   
\item Tar the files directly and rename the archive
\item Zip format is not accepted
\item Wrong submission: whoever submits your HW2 by mistake will get 0 and late policy will also apply if you resubmit it later
\end{itemize}
\end{document}
\end{document}